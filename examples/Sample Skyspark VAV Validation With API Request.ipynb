{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "global-recycling",
   "metadata": {},
   "source": [
    "# Sample Skyspark VAV Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-latitude",
   "metadata": {},
   "source": [
    "# 1) Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-international",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-woman",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Imports\n",
    "# ----------------------------------------\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "from rdflib import Namespace, SH, RDF, BNode, Graph\n",
    "from pyshacl import validate\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from tasty import constants as tc\n",
    "from tasty import graphs as tg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-store",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "Define the key variables and input information here\n",
    "\n",
    "***Items to Change***\n",
    "- `SHAPE`: this is the name of the SHACL equipment shape against which you would like to validate your sample equipment in the instance data\n",
    "- `SAMPLE`: this is the name of the sample equipment in your instance data\n",
    "- `input_namespace_uri`: this is the namespace uri used for your sample equipment in the instance data\n",
    "- `raw_data_graph_filename`: this is the filename/filepath to save the raw instance data (in turtle format) retrieved from the Skyspark API call\n",
    "- `data_graph_filename`: this is the filename/filepath to save the cleaned/processed instance data for the data graph to be used for validation\n",
    "- `shapes_graph_filename`: this it the filename/filepath of the SHACL shapes data for the shape graph \n",
    "***Remaining Items*** </br>\n",
    "These items should be okay as is, but can be changed if need be. If you are printing out results, <u>*make sure that the output directory exists in your local file structure*</u>.\n",
    "- `output_directory`: this is the directory where output files will be printed to below\n",
    "- `tasty_main_directory`: this is the absolute path of the main tasty directory. It should just be the parent directory of the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# User Defined Variables\n",
    "# ----------------------------------------\n",
    "\n",
    "SHAPE = 'NREL-VAV-SD-Cooling-Only-Shape'\n",
    "SAMPLE = '214466de-7abb28a7'\n",
    "input_namespace_uri = 'urn:/_#'\n",
    "\n",
    "raw_data_graph_filename = 'examples/output/sample_skyspark_vav_raw.ttl'\n",
    "data_graph_filename = 'examples/output/sample_skyspark_vav_clean.ttl'\n",
    "shapes_graph_filename = 'tasty/generated_shapes/haystack_all.ttl'\n",
    "\n",
    "output_directory = os.path.join(os.path.abspath(''), 'example_data/output')\n",
    "tasty_main_directory = os.path.join(os.path.abspath(''), '../')\n",
    "# print(tasty_main_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-accordance",
   "metadata": {},
   "source": [
    "## API Request From Skyspark \n",
    "NOTE - Must be connected to NREL network to access the api endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-savage",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "axon_query_string = '(point and equipRef->navName==\"UFVAV-3\") or (equip and navName==\"UFVAV-3\")'\n",
    "\n",
    "# load skyspark URL from .env file\n",
    "skyspark_api_url = os.environ.get('API_ENDPOINT')\n",
    "\n",
    "response = requests.get(\n",
    "    skyspark_api_url,\n",
    "    params={'filter':axon_query_string},\n",
    "    headers={\n",
    "        'Accept': 'text/turtle'\n",
    "    }   \n",
    ")\n",
    "\n",
    "print(response.status_code, end = \" - \")\n",
    "if response.status_code == 200:\n",
    "    print(\"Sucess\")\n",
    "elif response.status_code == 404:\n",
    "    print(\"Not Found\")\n",
    "\n",
    "# print(response.headers['Content-Type'])\n",
    "# response.encoding = 'utf-8'\n",
    "raw_skyspark_data = response.text\n",
    "print(raw_skyspark_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save response to file\n",
    "f = os.path.join(tasty_main_directory, raw_data_graph_filename)\n",
    "with open(f, 'w') as clean_file:\n",
    "    clean_file.write(raw_skyspark_data)\n",
    "print(f\"raw instance data saved to '{raw_data_graph_filename}' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-modern",
   "metadata": {},
   "source": [
    "### Pre-Process Raw Input File\n",
    "The raw data file generate by Skyspark cannot be parsed by rdflib as is, and also needs to be updated in a couple of other ways:\n",
    "1. The date-time fields generated by Skyspark cause errors in rdflibs parse function. These fields are removed\n",
    "2. The Skyspark namespace-prefix is and underscore \"\\_\" (this may be worth changing in the future), but more importantly this prefix is not associated with a URI namespace. The file will be parsed, however there is no way to reference the associated equipment as the namespace is not defined. The following prefix/namespace paring is added `@prefix _: <urn:/_#>`.\n",
    "3. Finally the project haystack namespaces that are listed in the raw file are version '3.9.9', however the shapes graph that is currently generated by tasty uses '3.9.10'. So the namespaces for the project haystack prefixes are updated to version 10.\n",
    "\n",
    "There are likely more elegant ways to handle these changes. For #3, we can likely generated a haystack version '3.9.9' graph to avoid this requirement. For #2 there is likely a way to bind this prefix to the graph prior to parsing the instance data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-driving",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Pre Process raw skyspark .ttl file \n",
    "# ----------------------------------------\n",
    "\n",
    "# read in the file\n",
    "f1 = os.path.join(tasty_main_directory, raw_data_graph_filename)\n",
    "with open(f1, 'r') as raw_file:\n",
    "    filedata = raw_file.read()\n",
    "\n",
    "# REMOVE DATE-TIME FIELDS\n",
    "#-------------------------\n",
    "# remove date-time fields in the middle of the definition\n",
    "filedata = re.sub('\\n.*\\^{2}xsd:dateTime.*;', '', filedata)\n",
    "# remove date-time fields at the end of the definition\n",
    "filedata = re.sub(';\\n.*\\^{2}xsd:dateTime.*.', '.', filedata)\n",
    "\n",
    "# add urn namespace to graph\n",
    "filedata = re.sub('@prefix', '@prefix _: <urn:/_#> .\\n@prefix', filedata, count = 1)\n",
    "\n",
    "# change the project haystack namespaces to v10\n",
    "filedata = re.sub('/3.9.9', '/3.9.10', filedata)\n",
    "\n",
    "# print(filedata)\n",
    "f2 = os.path.join(tasty_main_directory, data_graph_filename)\n",
    "with open(f2, 'w') as clean_file:\n",
    "    clean_file.write(filedata)\n",
    "print(f\"cleaned instance data saved to '{data_graph_filename}' \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-glass",
   "metadata": {},
   "source": [
    "# 2) Main Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-korean",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "This defines additional variables and helper functions to be used below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-thing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Variables and Constants\n",
    "# ----------------------------------------\n",
    "\n",
    "NAMESPACE = Namespace(input_namespace_uri)\n",
    "shape_name = tc.PH_SHAPES_NREL[SHAPE]\n",
    "target_node = NAMESPACE[SAMPLE]\n",
    "\n",
    "PHCUSTOM = Namespace(\"https://project-haystack.org/def/custom#\")\n",
    "POINT = Namespace(\"https://skyfoundry.com/def/point/3.0.27#\")\n",
    "\n",
    "# known_tags = [\"zone\", \"air\", \"temp\", \"sensor\", \"sp\", \"cmd\", \"discharge\", \"damper\", \"humidity\", \"co2\", \"occupied\",\n",
    "#                            \"occupancyIndicator\", \"cooling\", \"heating\", \"effective\", \"occ\", \"unocc\", \"standby\", \"operating\", \"mode\", \"request\",\n",
    "#                            \"leaving\", \"entering\", \"flow\", \"min\", \"max\", \"pressure\", ] \n",
    "\n",
    "# invalid_tags = [tc.PHIOT_3_9_10[\"point\"], tc.PHIOT_3_9_10[\"his\"], POINT[\"hisCollectCov\"], tc.PHIOT_3_9_10[\"cur\"]]\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helper Function Definitions\n",
    "# ----------------------------------------\n",
    "\n",
    "def get_data_graph():\n",
    "    n = tg.get_versioned_graph(tc.HAYSTACK, tc.V3_9_10)\n",
    "    f = os.path.join(tasty_main_directory, data_graph_filename)\n",
    "    n.parse(f, format='turtle')\n",
    "    return n\n",
    "\n",
    "\n",
    "def get_shapes_graph():\n",
    "    g = tg.get_versioned_graph(tc.HAYSTACK, tc.V3_9_10)\n",
    "    f = os.path.join(tasty_main_directory, shapes_graph_filename)\n",
    "    g.parse(f, format='turtle')\n",
    "    return g\n",
    "\n",
    "\n",
    "def print_graph_to_file(g, filename):\n",
    "    output_filename = os.path.join(output_directory, filename + \".ttl\")\n",
    "    g.serialize(output_filename, format='turtle')\n",
    "\n",
    "\n",
    "def print_graph(g):\n",
    "    print(g.serialize(format='turtle').decode('utf-8'))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-nutrition",
   "metadata": {},
   "source": [
    "## Generate and Process Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-interaction",
   "metadata": {},
   "source": [
    "### Create Data, Shapes, and Ontology Graphs \n",
    "Create the data and shapes graph using the helper functions defined above. The data and shapes graphs are generated using rdflib's `parse` function to import the graphs defined in `data_graph_filename` and the `shapes_graph_filename` respectively. The ontology graph is generated by the `load_ontology` method from tasty's `graphs` module (imported as `tg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-making",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Generate Graphs\n",
    "# ----------------------------------------\n",
    "\n",
    "# Data Graph\n",
    "data_graph = get_data_graph()\n",
    "print(\"...loaded data graph\")\n",
    "\n",
    "# Shapes Graph\n",
    "shapes_graph = get_shapes_graph()\n",
    "print(\"...loaded shapes graph\")\n",
    "\n",
    "# Ontology Graph\n",
    "ont_graph = tg.load_ontology(tc.HAYSTACK, tc.V3_9_10)\n",
    "print(\"...loaded ontology graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-china",
   "metadata": {},
   "source": [
    "### Get List of Valid Tags With Namespaces\n",
    "Create a list of valid tags with their prefixes, so we can remove all extraneous tags from the data graph (see below) </br>\n",
    "The methodology employed here is as follows:\n",
    "1. loop through each of the `.json` files in the `source-shapes/haystack` directory \n",
    "2. iterate over all shapes in each file\n",
    "3. extract any `tags` or `custom-tags` associated with each shape\n",
    "4. if the tag is not yet in the tags list, then it is added\n",
    "5. once all tags are added, generate a new list of namespaced tags (i.e. complete URIs for tags); iterate over the original list and for each tag use the `get_namespaced_term` method from tasty's `graphs` module (imported as `tg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-store",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Create a List of Valid Tags\n",
    "# ----------------------------------------\n",
    "source_shapes_dir = os.path.join(tasty_main_directory, 'tasty/source_shapes')\n",
    "schema = tc.HAYSTACK\n",
    "\n",
    "source_shapes_schema_dir = os.path.join(source_shapes_dir, schema.lower())\n",
    "files = [os.path.join(source_shapes_schema_dir, f) for f in \n",
    "         os.listdir(source_shapes_schema_dir) if f.endswith('.json')]\n",
    "\n",
    "valid_tags = []\n",
    "valid_tags_ns = []\n",
    "\n",
    "# go through schema files and extract valid tags\n",
    "for file in files:\n",
    "    # open file and read in json to python dict\n",
    "    with open(file, 'r') as f:\n",
    "        filedata = json.loads(f.read())\n",
    "        # for each shape \n",
    "        for shape in filedata['shapes']:\n",
    "            # add tags if not already added to the tags list\n",
    "            if 'tags' in shape:\n",
    "                for tag in shape['tags']:\n",
    "                    if tag not in valid_tags:\n",
    "                        valid_tags.append(tag)\n",
    "            # add custom tags if not already added to the tags list\n",
    "            if 'tags-custom' in shape:\n",
    "                for tag in shape['tags-custom']:\n",
    "                    if tag not in valid_tags:\n",
    "                            valid_tags.append(tag)\n",
    "        \n",
    "print(\"...generated tag list\")\n",
    "print(\"...adding namespaces\")\n",
    "# sort tags list\n",
    "valid_tags = sorted(valid_tags)\n",
    "\n",
    "# add namespaces to all valid tags\n",
    "false_count = 0\n",
    "for tag in valid_tags:\n",
    "    tag_ns = tg.get_namespaced_term(ont_graph, tag)\n",
    "    # take care of custom tags\n",
    "    if tag_ns is False:\n",
    "        tag_ns = PHCUSTOM[tag]\n",
    "    valid_tags_ns.append(tag_ns)\n",
    "    if(tag_ns == False):\n",
    "        false_count +=1\n",
    "    \n",
    "    print(f\"Tag: {tag:<20} URI: {tag_ns}\")\n",
    "\n",
    "print(false_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-ottawa",
   "metadata": {},
   "source": [
    "### Post Process of Data Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-research",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### (Ignore) Bind Default Namespace to Graph\n",
    "**Ignore this section; this issue was taken care of above during the raw-input pre-processing**\n",
    "\n",
    "The default namespace generated by skyspark appears to be an underscore \"\\_\". There is no prefix namespace associated with this on the output .ttl graph from skyspark, so there is no official URI. For the purpose of this excercise, we will use \"urn:\\/\\_#\", but we may wish to revisit this. The nasmespace is defined above under the \"Inputs\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-surname",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_graph.bind(\"_\", NAMESPACE)\n",
    "\n",
    "# # fix project-haystack namespaces\n",
    "# data_graph.bind(\"ph\", tc.PH_3_9_10, replace = True)\n",
    "# data_graph.bind(\"phScience\", tc.PHSCIENCE_3_9_10, replace = True)\n",
    "# data_graph.bind(\"phIoT\", tc.PHIOT_3_9_10, replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-transfer",
   "metadata": {},
   "source": [
    "#### a) Keep Only Valid Tags in Data Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-harmony",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep only valid tags\n",
    "for s1, p1, o1 in data_graph.triples((None, tc.PHIOT_3_9_10[\"equipRef\"], target_node)):\n",
    "    print(f\"...processing node: \\t{s1}\")\n",
    "    for s, p, o in data_graph.triples((s1, tc.PH_3_9_10[\"hasTag\"], None)):\n",
    "        if o not in valid_tags_ns:\n",
    "            data_graph.remove((s, p, o))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-sellers",
   "metadata": {},
   "source": [
    "#### b) Add First Class Point Type\n",
    "\n",
    "Had to modify this section - the turtle format that comes back from a get request is slightly different than the one tha comes back from the Skyspark UI </br>\n",
    "(as both of these are subclasses of \"point\" there should be a way to access it, but unclear what the RDF triple search critera should look like) </br>\n",
    "**Alternatively, we could look for everything with a point tag - but we have to do this before above step where we remove the 'point' tag** </br>\n",
    "NOTE: if 'his', 'cur', 'writable', or 'weather' tags are not removed, first class point may not be correct, as it assumes mutual exclusivity  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinct-blackberry",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tasty.skyspark import point_mapper as pm\n",
    "\n",
    "# load the point tree\n",
    "file = os.path.join(tasty_main_directory, 'tasty/schemas/haystack/defs_3_9_10.ttl')\n",
    "pt = pm.PointTree(file, 'point')\n",
    "root = pt.get_root()\n",
    "\n",
    "\n",
    "def clean_points(s, p, o ):\n",
    "    print(f\"Point: \\t{s}\")\n",
    "    print(f\"Tags: \", end = \"\")\n",
    "    \n",
    "    # get the tags for this point\n",
    "    tags = []\n",
    "    for s1,p1,o1 in data_graph.triples((s, tc.PH_3_9_10[\"hasTag\"], None)):\n",
    "        tag = o1[o1.find('#')+1:]\n",
    "        print(f\"\\t{tag}\")\n",
    "        tags.append(tag)\n",
    "    \n",
    "    # now determine first class point type\n",
    "    fc_point = pt.determine_first_class_point_type(root, tags)\n",
    "    print(f\"\\t...First Class Entity Type: {fc_point.type}\\n\")\n",
    "    \n",
    "    # add first class point type as class to the point\n",
    "    data_graph.add((s, RDF.type,tc.PHIOT_3_9_10[fc_point.type]))\n",
    "    # remove the tags associated with first class point\n",
    "    for tag in fc_point.tags:\n",
    "        # using all three namespaces because i do not know which is correct\n",
    "        # TODO: develop method for determining proper namespace\n",
    "        data_graph.remove((s, tc.PH_3_9_10[\"hasTag\"], tc.PHIOT_3_9_10[tag]))\n",
    "        data_graph.remove((s, tc.PH_3_9_10[\"hasTag\"], tc.PHSCIENCE_3_9_10[tag]))\n",
    "        data_graph.remove((s, tc.PH_3_9_10[\"hasTag\"], tc.PH_3_9_10[tag]))    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get all points with an equipRef\n",
    "for s, p, o in data_graph.triples((None, tc.PHIOT_3_9_10[\"equipRef\"], None)):\n",
    "    clean_points(s,p,o)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-devil",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_graph(data_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-pound",
   "metadata": {},
   "source": [
    "### Add Sample Equipment as Target Node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-gibson",
   "metadata": {},
   "source": [
    "First we add a triple to the shapes graph:\n",
    "- The **subject** is the SHACL equipment shape\n",
    "- The **predicate** is `sh:targetNode`\n",
    "- The **object** is the sample equipment\n",
    "\n",
    "This indicates that the sample shape should conform to the overall SHACL equipment shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Instance Equipment as target node to SHACL Equipment Shape\n",
    "shapes_graph.add((shape_name, SH.targetNode, target_node))\n",
    "print(f\"\\tadded '{target_node}' as target node to {shape_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-poverty",
   "metadata": {},
   "source": [
    "Next we iterate over all *nodes* of the SHACL equipment shape using rdflidb's `triples()` function which supports basic triple pattern matching ([see documentation here](https://rdflib.readthedocs.io/en/stable/intro_to_graphs.html)). For each triple with a subject of the SHACL equipment shape and predicate of `sh:node`, we take the object (i.e. all of the functional group shapes which constitute the equipment shape) and add the sample equipment as a target node to these shapes. This is done so that the validation results will identify specific points that fail to validate, rather than simply functional group shapes.</br>\n",
    "So for each *node* (functional group shape) add a triple to the shapes graph:\n",
    "- The **subject** is the *node* (functional group shape)\n",
    "- The **predicate** is `sh:targetNode`\n",
    "- The **object** is the sample equipment\n",
    "\n",
    "Ultimately, this means we are indicating that the sample equipment should conform to each of these functional group shapes independently. Note that this is acceptable currently because there is no `maxCount` on the functional group shape's `equipRef` path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-circulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add Instance Equipment as target node to SHACL Functional Groups Shapes\n",
    "for s1, p1, o1 in shapes_graph.triples((shape_name, SH.node, None)):\n",
    "    shapes_graph.add((o1, SH.targetNode, target_node))\n",
    "    print(f\"\\tadded '{target_node}' as target node to {o1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-major",
   "metadata": {},
   "source": [
    "# 3) Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subject-yellow",
   "metadata": {},
   "source": [
    "## PySHACL Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-blackjack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Run pySCHACL Validation\n",
    "# ----------------------------------------\n",
    "result = validate(data_graph, shacl_graph=shapes_graph, ont_graph=ont_graph)\n",
    "conforms, results_graph, results = result\n",
    "\n",
    "print(f\"Conforms: {conforms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-demand",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_graph(results_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-valve",
   "metadata": {},
   "source": [
    "## Determine Missing Points\n",
    "This implements a rudimentary logic for finding the missing points (simple shapes) from the pySHACL results graph. The process is as follows:\n",
    "1. Find each \"validation result\" which represents one SHACL constraint that was not met. This is done by iterating through all the triples in the graph and finding the triple with a `rdf:type` of `sh:ValidationResult`. The subject of this match will be the URI of the \"validation result\" node.\n",
    "2. For each of these \"validation results\" look at the `sh:sourceShape`\n",
    "3. If it is a BNode (as opposed to a URI) then we assume this refers to one of the constraints on the functional group SHACL shape (and therefore one of the \"simple shapes\") and it will have a `sh:qualifiedValueShape` which should be a URI of one of the simple shapes.\n",
    "4. Add this shape to the list of missing points\n",
    "\n",
    "*Note: this logic likely needs to be refined*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-merchant",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing_points = []\n",
    "optional_points = []\n",
    "\n",
    "# Find the Validation Results\n",
    "for subject, predicate, object in results_graph.triples((None, RDF.type, SH.ValidationResult)):\n",
    "#     print(f\"Subject:{subject}\\tPredicate:{predicate}\\tObject:{object}\")\n",
    "    severity = results_graph.value(subject = subject, predicate= SH.resultSeverity)\n",
    "    \n",
    "    # check if Validation result points to a BNode\n",
    "    for node in results_graph.objects(subject=subject, predicate=SH.sourceShape):\n",
    "#         print(f\"\\tNode:{node}\\t\\tIs BNode:{isinstance(node, BNode)}\")\n",
    "\n",
    "        if isinstance(node, BNode):\n",
    "            point = results_graph.value(subject=node, predicate=SH.qualifiedValueShape)\n",
    "            \n",
    "            if(severity == SH.Violation):\n",
    "                missing_points.append(point)\n",
    "            elif(severity == SH.Warning):\n",
    "                optional_points.append(point)\n",
    "if len(missing_points) <= 0:\n",
    "    print(\"No Points Missing\")\n",
    "else:\n",
    "    print(f\"{len(missing_points)} Missing Points:\")\n",
    "    for point in missing_points:\n",
    "#         for subject, predicate, object in shapes_graph.triples((point, SH.class, None)):\n",
    "#             print(f\"Subject:{subject}\\tPredicate:{predicate}\\tObject:{object}\")\n",
    "        print(f\"\\t{point}\")\n",
    "    \n",
    "if len(optional_points) <= 0:\n",
    "    print(\"No Optional Points Missing\")\n",
    "else:\n",
    "    print(f\"{len(optional_points)} Missing Optional Points:\")\n",
    "    for point in optional_points:\n",
    "        print(f\"\\t{point}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-alert",
   "metadata": {},
   "source": [
    "## Print pySHACL Graphs and Results to File (Optional) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-renaissance",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Print Output Files\n",
    "# ----------------------------------------\n",
    "# Print Results to File\n",
    "fn = os.path.join(output_directory, \"results.txt\")\n",
    "f = open(fn, \"w\")\n",
    "f.write(results)\n",
    "f.close()\n",
    "print(\"...printed results\")\n",
    "\n",
    "# Print Graphs to File(s)\n",
    "print_graph_to_file(data_graph, \"data_graph\")\n",
    "print(\"...printed data graph\")\n",
    "print_graph_to_file(shapes_graph, \"shapes_graph\")\n",
    "print(\"...printed shapes graph\")\n",
    "print_graph_to_file(results_graph, \"results_graph\")\n",
    "print(\"...printed results graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-colonial",
   "metadata": {},
   "source": [
    "## 3b) Brick Validation (Optional)\n",
    "Brickscehma uses pyshacl for validation, so it gives us the same result. In this case, we just passed in the shapes graph directly, so this is not actually testing conformance against an actual brick model or using the brick schema in any significant way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-heart",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Run BrickSchema Validation\n",
    "# ----------------------------------------\n",
    "\n",
    "from brickschema import Graph\n",
    "\n",
    "# Set Up Graphs\n",
    "dg = Graph()\n",
    "df = os.path.join(tasty_main_directory, data_graph_filename)\n",
    "dg.load_file(df)\n",
    "\n",
    "sg = Graph()\n",
    "sf = os.path.join(tasty_main_directory, shapes_graph_filename)\n",
    "sg.load_file(sf)\n",
    "\n",
    "valid, _, report = dg.validate(shape_graphs=[sg])\n",
    "print(f\"Brick Validation - Conforms: {valid}\")\n",
    "if not valid:\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-generation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
